{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"tokenizer.model\"\n",
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.load(tokenizer_path)\n",
    "vocab_size = tokenizer.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/TinyStories-valid.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    # for line in file:\n",
    "    #     # Process each line here\n",
    "    #     print(f\"1.{line.strip()}\") \n",
    "\n",
    "string_to_remove = '<|endoftext|>\\n'\n",
    "result_list = [x for x in lines if x != string_to_remove]\n",
    "lines = \"\".join(result_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(lines)))\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_CONFIG = {\n",
    "    \"vocab_size\": len(vocab),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5085262])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = torch.tensor(tokenizer.encode(lines))\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loader:\n",
    "from torch.utils.data import Dataset\n",
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, context_window=100, batch_size=32):\n",
    "        \"\"\" Args:\n",
    "        data (str): the dataset\n",
    "        vectorizer (ReviewVectorizer): vectorizer instantiated from dataset \"\"\"\n",
    "        self.data = data \n",
    "        self._tokenizer = tokenizer\n",
    "        self.context_window = context_window\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "        # the data is a large string, need to split it into train, val, and test based on periods\n",
    "        # self.data = self.data.split('.')\n",
    "        self.data = torch.tensor(self._tokenizer.encode(self.data))\n",
    "\n",
    "\n",
    "        self.train_data = self.data[:int(.8 * len(self.data))]\n",
    "        self.train_size = len(self.data[:int(.8 * len(self.data))])\n",
    "\n",
    "        self.val_data = self.data[int(.8 * len(self.data)): int(.9 * len(self.data))]\n",
    "        self.val_size = len(self.data[int(.8 * len(self.data)): int(.9 * len(self.data))])\n",
    "\n",
    "        self.test_data = self.data[int(.9 * len(self.data)):]\n",
    "        self.test_size = len(self.data[int(.9 * len(self.data)):])\n",
    "\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_data, self.train_size), \n",
    "                             'val': (self.val_data, self.val_size),\n",
    "                             'test': (self.test_data, self.test_size)} \n",
    "        self.set_split('train')\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_tokenizer(cls, data_path, tokenizer_path):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        Args:\n",
    "        review_csv (str): location of the dataset\n",
    "        Returns:\n",
    "        an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        #load and slightly preprocess the dataset\n",
    "        with open(data_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        string_to_remove = '<|endoftext|>\\n'\n",
    "        result_list = [x for x in lines if x != string_to_remove]\n",
    "        lines = \"\".join(result_list)\n",
    "        # load the tokenizer\n",
    "        tokenizer = SentencePieceProcessor()\n",
    "        tokenizer.load(tokenizer_path)\n",
    "\n",
    "        return cls(lines, tokenizer)\n",
    "    \n",
    "    def get_tokenizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._tokenizer\n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe\n",
    "        Args:\n",
    "        split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_data, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        Args:\n",
    "        index (int): the index to the data point\n",
    "        Returns:\n",
    "        a dict of the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        # row = self._target_data.iloc[index]\n",
    "        # review_vector = self._vectorizer.vectorize(row.review)\n",
    "        # rating_index =  self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "\n",
    "\n",
    "           # pick random starting points\n",
    "        ix = torch.randint(0, self._target_data.size(0) - self.context_window - 1, (self.batch_size,))\n",
    "        x = torch.stack([self._target_data[i:i+self.context_window] for i in ix]).long()\n",
    "        y = torch.stack([self._target_data[i+1:i+self.context_window+1] for i in ix]).long()\n",
    "        return x, y\n",
    "        # return {'x_data': review_vector,\n",
    "        #         'y_target': rating_index}\n",
    "    \n",
    "    def get_num_batches(self):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        Args:\n",
    "        batch_size (int)\n",
    "        Returns:\n",
    "        number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate batches:\n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "    ensure each tensor is on the write device location. \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device) \n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StoryDataset.load_dataset_and_tokenizer(file_path, tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  526,   366, 20413,  ...,   322,  1494,   372],\n",
       "        [ 6788,   322,   278,  ...,  4111,   526,  3252],\n",
       "        [  278, 12580, 29880,  ..., 22804,   471,   263],\n",
       "        ...,\n",
       "        [29889,  2688,   437,  ...,   940,   947,   451],\n",
       "        [  367, 26230, 29889,  ...,   304,  1708,   411],\n",
       "        [  508,  2125,   372,  ...,   472,  1009,  1856]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
